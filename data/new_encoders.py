from tokenizers import Tokenizer, trainers, models, pre_tokenizers
from tokenizers.normalizers import NFD, Lowercase, StripAccents, Sequence
from tokenizers.pre_tokenizers import Whitespace
import json
import numpy as np
from pathlib import Path

current_file = Path(__file__).resolve()
data_dir = current_file.parent
config_file = data_dir.parent / "config" / "config.json"
raw_dir = data_dir / "raw"
processed_dir = data_dir / "processed"
processed_dir.mkdir(parents=True, exist_ok=True)

# ƒê·ªçc config ƒë·ªÉ l·∫•y max_seq_len
with open(config_file, 'r') as f:
    config = json.load(f)
max_seq_len = config['max_seq_len']
vocab_size = config['vocab_size']

# B∆∞·ªõc 1: T·∫£i d·ªØ li·ªáu
dataset = []
with open(raw_dir / "pre_train.json", "r", encoding="utf-8") as f:
    json_data = json.load(f)
    dataset = [item.strip() for item in json_data if isinstance(item, str) and item.strip()]

# B∆∞·ªõc 2: T·∫°o tokenizer BPE
tokenizer = Tokenizer(models.BPE())
tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])
tokenizer.pre_tokenizer = Whitespace()
trainer = trainers.BpeTrainer(
    vocab_size=vocab_size, min_frequency=2,
    special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]", "[BOS]", "[EOS]"]
)
tokenizer.train_from_iterator(dataset, trainer=trainer)

# B∆∞·ªõc 3: L∆∞u tokenizer v√† vocab
tokenizer.save(str(processed_dir / "bpe_tokenizer.json"))

vocab = tokenizer.get_vocab()
sorted_vocab = sorted(vocab.items(), key=lambda x: x[1])
with open(data_dir / "new_vocab.txt", 'w', encoding='utf-8') as f:
    for token, idx in sorted_vocab:
        f.write(f"{token}\t{idx}\n")

# B∆∞·ªõc 4: Tokenize v√† t·∫°o X, Y, lengths (c√πng format v·ªõi VnCoreNLP)
X, Y, lengths = [], [], []

for line in dataset:
    encoded = tokenizer.encode(line.lower())
    tokens = encoded.ids
    
    # B·ªè qua c√¢u qu√° ng·∫Øn ho·∫∑c qu√° d√†i (gi·ªëng logic VnCoreNLP)
    if len(tokens) < 2 or len(tokens) > max_seq_len - 2:  # -2 ƒë·ªÉ d√†nh ch·ªó cho BOS/EOS n·∫øu c·∫ßn
        continue
    
    # KH√îNG padding ·ªü ƒë√¢y - ƒë·ªÉ train.py x·ª≠ l√Ω dynamic padding
    X.append(tokens)
    Y.append(tokens)  # Y c≈©ng l√† tokens (t∆∞∆°ng t·ª± nh∆∞ VnCoreNLP)
    lengths.append(len(tokens))

# B∆∞·ªõc 5: L∆∞u c√πng format v·ªõi VnCoreNLP
np.savez_compressed(
    processed_dir / "new_data_tokenized.npz",
    X=np.array(X, dtype=object),
    Y=np.array(Y, dtype=object),
    lengths=np.array(lengths)
)

print(f"‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu v√†o: {processed_dir}/new_data_tokenized.npz")
print(f"üìä T·ªïng s·ªë m·∫´u: {len(X)}")
print(f"üìà ƒê·ªô d√†i sequence trung b√¨nh: {np.mean(lengths):.2f}")
print(f"üìâ ƒê·ªô d√†i sequence min/max: {min(lengths)}/{max(lengths)}")