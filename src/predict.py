import tensorflow as tf
import numpy as np
from keras import models
import json
import sys
from pathlib import Path
from vncorenlp import VnCoreNLP
project_root = Path(__file__).resolve().parent.parent
sys.path.append(str(project_root))

current_file = Path(__file__).resolve()

model_path = project_root / "model" / "s_a_i.keras"
model = models.load_model(model_path)

config_dir = current_file.parent.parent / "config" / "config.json"
with open(config_dir, 'r') as f:
    config = json.load(f)
max_seq_len = config['max_seq_len']

# ƒê·ªçc vocab
vocab = {}
vocab_path = current_file.parent.parent/ "data" / "vocab.txt"
with open(vocab_path, "r", encoding="utf-8") as f:
    for line in f:
        word, idx = line.strip().split('\t')
        vocab[word] = int(idx)

idx2word = {i: w for w, i in vocab.items()}

VNCORENLP_PATH = "/Users/thongbui.nd/vncorenlp/VnCoreNLP/VnCoreNLP-1.1.1.jar"
annotator = VnCoreNLP(VNCORENLP_PATH, annotators="wseg", max_heap_size='-Xmx2g')

def tokenize(sentence):
    """Chuy·ªÉn ƒë·ªïi c√¢u th√†nh token s·ªë, s·ª≠ d·ª•ng VnCoreNLP ƒë·ªÉ t√°ch t·ª´ ti·∫øng Vi·ªát"""
    word_segments = annotator.tokenize(sentence.lower())
    words = [word for segment in word_segments for word in segment]
    tokens = [vocab.get(w, vocab["[UNK]"]) for w in words]
    return tokens

def detokenize(tokens, infor=None):
    """Chuy·ªÉn token s·ªë v·ªÅ c√¢u vƒÉn b·∫£n, thay th·∫ø token ƒë·∫∑c bi·ªát n·∫øu c·∫ßn"""
    special_tokens = {0, 1, 2, 3, 4, 5, 6}  # PAD, UNK, BOS, EOS, SEP
    words = []
    for t in tokens:
        if t in special_tokens or t not in idx2word:
            continue
        word = idx2word[t]
        if infor and word in infor:
            words.append(infor[word])
        else:
            words.append(word)
    return " ".join(words)

def old_generate_response(sentence, max_new_tokens=32, infor=None):
    """T·∫°o ph·∫£n h·ªìi d·ª±a tr√™n th√¥ng tin c√° nh√¢n - AUTOREGRESSIVE GENERATION"""
    req_tokens = tokenize(sentence)
    
    # B·∫Øt ƒë·∫ßu v·ªõi [BOS] + request
    current_sequence = [vocab["[BOS]"]] + req_tokens
    
    # Generate t·ª´ng token m·ªôt
    for step in range(max_new_tokens):
        # Pad sequence ƒë·ªÉ fit model
        padded_input = tf.keras.preprocessing.sequence.pad_sequences(
            [current_sequence], maxlen=max_seq_len, padding='post', dtype='int32'
        )
        
        # Predict next token
        preds = model(padded_input, training=False)
        
        # L·∫•y token ·ªü v·ªã tr√≠ cu·ªëi sequence th·∫≠t
        pos1 = len(current_sequence) - 1
        if pos1 < preds.shape[1]:
            next_token_probs1 = preds[0, pos1, :]
            next_token1 = np.argmax(next_token_probs1)
            next_token = int(next_token1)  # √âp ki·ªÉu th√†nh int
        else:
            next_token = vocab["[EOS]"]
        
        # D·ª´ng n·∫øu g·∫∑p EOS ho·∫∑c PAD
        if next_token == vocab["[EOS]"] or next_token == vocab["[PAD]"]:
            break
            
        current_sequence.append(next_token)
        
        # Tr√°nh sequence qu√° d√†i
        if len(current_sequence) >= max_seq_len:
            break
    
    # Tr·∫£ v·ªÅ to√†n b·ªô sequence (b·ªè [BOS])
    return detokenize(current_sequence[1:], infor)

# ================
# Ki·ªÉm Tra M√¥ H√¨nh
# ================

prompts = [
    "b√°nh m√¨",
    "b√°nh m√¨ c√≥ ngu·ªìn g·ªëc t·ª´",
    "vi·ªát nam",
    "vi·ªát nam s·ªü h·ªØu",
    "ph·ªü",
    "bu·ªïi s√°ng ng∆∞·ªùi vi·ªát nam th∆∞·ªùng ƒÉn",
    "ƒë√°m m√¢y",
    "ƒêinh Ti√™n Ho√†ng l√™n ng√¥i",
    "l√™ th√°i t·ªï c√≥ mi·∫øu hi·ªáu",
    "c√¥ng th·ª©c 1",
    "s√°ng h√¥m ·∫•y",
    "sau khi ƒÉn xong, ch√∫ng t√¥i ƒëi",
    "m·∫∑c d√π",
    "b·ªüi v√¨ tr·ªùi m∆∞a,"
]

print("\n=== Test pre-train ===")
for req in prompts:
    print(f"Req: {req} \nRes: {old_generate_response(req)}")

def new_generate_response(sentence, max_new_tokens=max_seq_len, top_k=3, temperature=1.0, verbose=False):
    """
    T·∫°o ph·∫£n h·ªìi t·ª´ c√¢u ƒë·∫ßu v√†o.
    current_sequence = [BOS] + req
    sequence = loop(predict(current_sequence))
    N√¢ng c·∫•p:
    - T·ªëi ∆∞u ho√° Padding
    - ƒêa d·∫°ng c∆° ch·∫ø l·∫•y m·∫´u (sampling), s·ª≠ d·ª•ng Top-k
    - Thay v√¨ ch·ªâ s·ª≠ d·ª•ng token cu·ªëi th√¨ s·ª≠ d·ª•ng c·∫£ ƒëo·∫°n t·ª´ ƒë·∫ßu ƒë·ªÉ d·ª± ƒëo√°n
    """
    req_tokens = tokenize(sentence)
    current_sequence = [vocab["[BOS]"]] + req_tokens

    padded_input = tf.keras.preprocessing.sequence.pad_sequences(
        [current_sequence], maxlen=max_seq_len, padding='post', dtype='int32'
    )

    if verbose:
        print(f"üîß TH√îNG S·ªê SINH VƒÇN B·∫¢N:")
        print(f"   üìù C√¢u ƒë·∫ßu v√†o: '{sentence}'")
        print(f"   üéØ Max tokens: {max_new_tokens}")
        print(f"   üî• Temperature: {temperature}")
        print(f"   üé≤ Top-k: {top_k}")
        print(f"   üìä ƒê·ªô d√†i sequence ban ƒë·∫ßu: {len(current_sequence)}")
        print(f"\n{'='*60}")

    for step in range(max_new_tokens):
        preds = model(padded_input, training=False)
        next_token_probs = preds[0, len(current_sequence) - 1, :].numpy()

        # √Åp d·ª•ng temperature
        next_token_probs = np.exp(np.log(next_token_probs + 1e-10) / temperature)
        next_token_probs /= np.sum(next_token_probs)

        # Top-k sampling
        top_k_indices = np.argsort(next_token_probs)[-top_k:]
        top_k_probs = next_token_probs[top_k_indices] / np.sum(next_token_probs[top_k_indices])
        next_token = np.random.choice(top_k_indices, p=top_k_probs)

        # Hi·ªÉn th·ªã th√¥ng tin n·∫øu verbose=True
        if verbose:
            selected_prob = next_token_probs[next_token]
            loss = -np.log(selected_prob + 1e-10)
            
            print(f"\nüîÑ B∆Ø·ªöC {step + 1}:")
            print(f"   üìç Token ƒë∆∞·ª£c ch·ªçn: ID {next_token}")
            print(f"   üìâ Loss: {loss:.4f}")
            print(f"   üèÜ TOP 3 ·ª®NG VI√äN:")
            
            # Hi·ªÉn th·ªã top 3 v·ªõi loss
            top_3_indices = np.argsort(next_token_probs)[-3:][::-1]
            for i, idx in enumerate(top_3_indices):
                prob = next_token_probs[idx]
                token_loss = -np.log(prob + 1e-10)
                is_selected = "‚úÖ" if idx == next_token else "  "
                print(f"      {is_selected} #{i+1}: ID {idx} (p={prob:.4f}, loss={token_loss:.4f})")

        if next_token in [vocab["[EOS]"], vocab["[PAD]"]]:
            break

        current_sequence.append(int(next_token))
        padded_input[0, len(current_sequence) - 1] = next_token

        if len(current_sequence) >= max_seq_len:
            break
    
    return detokenize(current_sequence[1:])

print("\n=== Test pre-train ===")
for req in prompts:
    print(f"Req: {req} \nRes: {new_generate_response(req)}")


